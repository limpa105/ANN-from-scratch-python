{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Pytorch_SequentialvsNested.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMFkRymHC9aX0hBbROT1eo2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/limpa105/ANN-from-scratch-python/blob/main/Pytorch_SequentialvsNested.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dh2RGDadren3"
      },
      "source": [
        "import torch\n",
        "import torchvision\n",
        "from torch.utils import data\n",
        "from torchvision import transforms\n",
        "from torch import nn\n",
        "\n",
        "import matplotlib.pyplot as plt \n",
        "import numpy as np"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3EyCiTJm_PS9"
      },
      "source": [
        "# Main Goal: Learn more about Pytorch \n",
        "# Research Question #1: Can I implement a very simple pytorch neural network with three linear layers that uses \n",
        "# both sequential and custom created nested architecture and succesfully learns paters in the \n",
        "# Fashion minst data ?"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DUpeA1R4rBgY"
      },
      "source": [
        "# Function to load fashion minst data \n",
        "def load_fashion_mnist(batch_size: int = 512, num_workers: int = 4):\n",
        "    data_transform = transforms.ToTensor() # Obtaining data to tensor converter\n",
        "    \n",
        "    # Downloading data\n",
        "    mnist_train = torchvision.datasets.FashionMNIST(root = \"../data\", train = True, transform = data_transform, download= True)  # Defining fashion MNIST train from torch datasets\n",
        "    mnist_test = torchvision.datasets.FashionMNIST(root = \"../data\", train = False, transform = data_transform, download = True)\n",
        "    \n",
        "    # Loading data onto an iterator\n",
        "    train_data_loader = data.DataLoader(mnist_train, batch_size, shuffle = True, num_workers = 4)\n",
        "    test_data_loader = data.DataLoader(mnist_test, batch_size, shuffle = True, num_workers = 4)\n",
        "    \n",
        "    # Returning iterator\n",
        "    return train_data_loader, test_data_loader"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ku9lfwclrpdy",
        "outputId": "081c5bb4-9e96-419c-f8b5-902a817e2376"
      },
      "source": [
        "train_iter, test_iter = load_fashion_mnist()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torchvision/datasets/mnist.py:498: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /pytorch/torch/csrc/utils/tensor_numpy.cpp:180.)\n",
            "  return torch.from_numpy(parsed.astype(m[2], copy=False)).view(*s)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yMzoH0BOse9G"
      },
      "source": [
        "# one way to implement a neural network\n",
        "neural_network = torch.nn.Sequential(torch.nn.Flatten(), \n",
        "                            torch.nn.Linear(784, 196), \n",
        "                            torch.nn.Sigmoid(),\n",
        "                            torch.nn.Dropout(0.1),\n",
        "                            torch.nn.Linear(196,49),\n",
        "                            torch.nn.Sigmoid(),\n",
        "                            torch.nn.Dropout(0.3),\n",
        "                            torch.nn.Linear(49,10),\n",
        "                            torch.nn.Softmax())"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O-8bSYJyszNK"
      },
      "source": [
        "# Defining a trainer\n",
        "trainer = torch.optim.Adam(neural_network.parameters(),weight_decay= 0.00001, lr = 0.001)\n",
        "loss = torch.nn.CrossEntropyLoss()"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C95AVVgZsw4y",
        "outputId": "fbdee495-0323-4e84-b587-6fce6e7bbfcb"
      },
      "source": [
        "def init_weights_biases(layer: torch.nn):\n",
        "    # Linear is the only layer wirh weights as others are drop out and activation \n",
        "    # functions\n",
        "    if isinstance(layer, torch.nn.Linear):\n",
        "      # Weights picked from normal distribution by random \n",
        "      # _ automatically allows for grad\n",
        "        torch.nn.init.normal_(layer.weight, mean = 0, std = 0.1) \n",
        "        torch.nn.init.zeros_(layer.bias)   \n",
        "\n",
        "neural_network.apply(init_weights_biases) # Autoinitialize any linear weights with normal inputs"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Sequential(\n",
              "  (0): Flatten(start_dim=1, end_dim=-1)\n",
              "  (1): Linear(in_features=784, out_features=196, bias=True)\n",
              "  (2): Sigmoid()\n",
              "  (3): Dropout(p=0.1, inplace=False)\n",
              "  (4): Linear(in_features=196, out_features=49, bias=True)\n",
              "  (5): Sigmoid()\n",
              "  (6): Dropout(p=0.3, inplace=False)\n",
              "  (7): Linear(in_features=49, out_features=10, bias=True)\n",
              "  (8): Softmax(dim=None)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZlgCKzTM1Uiu",
        "outputId": "7b3b7090-e440-43f8-cf6a-52e62e69b5df"
      },
      "source": [
        "print(neural_network[7].weight.data[0])\n",
        "print(neural_network[7].bias.data[0])"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([-0.4871, -0.4730, -0.5370, -0.1373,  0.3936, -0.4183, -0.3490, -0.4284,\n",
            "        -0.1007,  0.2498,  0.3874,  0.3715,  0.2142, -0.5159, -0.2314, -0.3498,\n",
            "        -0.3717,  0.3227,  0.3721,  0.4309,  0.3960,  0.3070, -0.3435,  0.4418,\n",
            "         0.4499, -0.4788, -0.2172,  0.3336,  0.3968,  0.3722,  0.2015, -0.3938,\n",
            "        -0.3583, -0.3152, -0.4644,  0.4404,  0.2873,  0.3441, -0.3815, -0.3617,\n",
            "        -0.2917, -0.4279,  0.4304,  0.3559, -0.3837,  0.3245,  0.4286, -0.3525,\n",
            "         0.4385])\n",
            "tensor(-0.0099)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cPyGyvmWss5h",
        "outputId": "208a4466-9257-4cc8-de73-fb708a0cd307"
      },
      "source": [
        "num_epochs = 10\n",
        "neural_network.train()\n",
        "for epoch in range(num_epochs):\n",
        "    for data_point, label_point in train_iter: # Traversing data loader\n",
        "        trainer.zero_grad() # Starting by resetting gradient of trainer\n",
        "        cost = loss(neural_network(data_point), label_point) # Computing cost\n",
        "        cost.backward() # Backwards propagating\n",
        "        trainer.step() # performing step\n",
        "    print(f'The loss for epoch {epoch} is {cost.sum()}')"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/container.py:139: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  input = module(input)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "The loss for epoch 0 is 1.6968698501586914\n",
            "The loss for epoch 1 is 1.6428083181381226\n",
            "The loss for epoch 2 is 1.7424696683883667\n",
            "The loss for epoch 3 is 1.7007699012756348\n",
            "The loss for epoch 4 is 1.5984997749328613\n",
            "The loss for epoch 5 is 1.599530577659607\n",
            "The loss for epoch 6 is 1.6011695861816406\n",
            "The loss for epoch 7 is 1.595845341682434\n",
            "The loss for epoch 8 is 1.5931254625320435\n",
            "The loss for epoch 9 is 1.6245924234390259\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1UcCI4HRyBao",
        "outputId": "de4eb5d5-9655-48ba-d61b-7f64b1cde03e"
      },
      "source": [
        "#making sure the weights and biases were changed by the model \n",
        "print(neural_network[7].weight.data[0])\n",
        "print(neural_network[7].bias.data[0])"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([-0.4871, -0.4730, -0.5370, -0.1373,  0.3936, -0.4183, -0.3490, -0.4284,\n",
            "        -0.1007,  0.2498,  0.3874,  0.3715,  0.2142, -0.5159, -0.2314, -0.3498,\n",
            "        -0.3717,  0.3227,  0.3721,  0.4309,  0.3960,  0.3070, -0.3435,  0.4418,\n",
            "         0.4499, -0.4788, -0.2172,  0.3336,  0.3968,  0.3722,  0.2015, -0.3938,\n",
            "        -0.3583, -0.3152, -0.4644,  0.4404,  0.2873,  0.3441, -0.3815, -0.3617,\n",
            "        -0.2917, -0.4279,  0.4304,  0.3559, -0.3837,  0.3245,  0.4286, -0.3525,\n",
            "         0.4385])\n",
            "tensor(-0.0099)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W-QR9JvI5DOD"
      },
      "source": [
        "# Lets try nesting some archiecture and see if we can make the dimensions align and \n",
        "# results make sense\n",
        "class Net(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.one = torch.nn.Sequential(torch.nn.Flatten(), \n",
        "                            torch.nn.Linear(784, 196), \n",
        "                            torch.nn.Sigmoid(),\n",
        "                            torch.nn.Dropout(0.2))\n",
        "    self.two = torch.nn.Linear(196,49)\n",
        "\n",
        "  def forward(self, X):\n",
        "    return self.two(self.one(X))"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ygOQjHp28Uy5",
        "outputId": "d6f80cf1-9e7b-402d-a8a2-5311c20fa9e2"
      },
      "source": [
        "second_network = nn.Sequential(Net(), nn.Dropout(0.3),nn.Sigmoid(), nn.Linear(49,10), nn.Softmax())\n",
        "second_trainer = torch.optim.Adam(second_network.parameters(),weight_decay= 0.00001, lr = 0.001)\n",
        "second_network.apply(init_weights_biases)\n",
        "\n",
        "num_epochs = 10\n",
        "neural_network.train()\n",
        "for epoch in range(num_epochs):\n",
        "    for data_point, label_point in train_iter: # Traversing data loader\n",
        "        second_trainer.zero_grad() # Starting by resetting gradient of trainer\n",
        "        cost = loss(second_network(data_point), label_point) # Computing cost\n",
        "        cost.backward() # Backwards propagating\n",
        "        second_trainer.step() # performing step\n",
        "    print(f'The loss for epoch {epoch} is {cost.sum()}')"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/container.py:139: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  input = module(input)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "The loss for epoch 0 is 2.017996311187744\n",
            "The loss for epoch 1 is 1.8924640417099\n",
            "The loss for epoch 2 is 1.8067398071289062\n",
            "The loss for epoch 3 is 1.7174171209335327\n",
            "The loss for epoch 4 is 1.7138175964355469\n",
            "The loss for epoch 5 is 1.7823234796524048\n",
            "The loss for epoch 6 is 1.7235075235366821\n",
            "The loss for epoch 7 is 1.646540641784668\n",
            "The loss for epoch 8 is 1.670068383216858\n",
            "The loss for epoch 9 is 1.6064082384109497\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g9doBEHI9i11",
        "outputId": "b41b59dd-1745-4a8b-de65-ca131773c509"
      },
      "source": [
        "print(second_network)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sequential(\n",
            "  (0): Net(\n",
            "    (one): Sequential(\n",
            "      (0): Flatten(start_dim=1, end_dim=-1)\n",
            "      (1): Linear(in_features=784, out_features=196, bias=True)\n",
            "      (2): Sigmoid()\n",
            "    )\n",
            "    (two): Linear(in_features=196, out_features=49, bias=True)\n",
            "  )\n",
            "  (1): Sigmoid()\n",
            "  (2): Linear(in_features=49, out_features=10, bias=True)\n",
            "  (3): Softmax(dim=None)\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ex6MvLzY9-rf",
        "outputId": "c80005ee-d21d-47e3-e31d-d4b064f9b1be"
      },
      "source": [
        "# Check if \n",
        "print(sum(p.numel() for p in neural_network.parameters() if p.requires_grad))\n",
        "print(sum(p.numel() for p in second_network.parameters() if p.requires_grad))"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "164013\n",
            "164013\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uOmK197EAYCZ",
        "outputId": "2cf6e047-8317-4e81-d284-e2708d14a15d"
      },
      "source": [
        "images, labels = iter(train_iter).next()"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "id": "g4bS3NjfApZ4",
        "outputId": "da19f6d5-9878-4744-9a5b-63328d39f6e5"
      },
      "source": [
        "plt.imshow(images[11].squeeze(), cmap='gray')"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f3223f57390>"
            ]
          },
          "metadata": {},
          "execution_count": 44
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yVd9udRdBEsO",
        "outputId": "28999e1a-e180-4bbd-85af-d52cdc771fc4"
      },
      "source": [
        "neural_network.eval()\n",
        "print(np.argmax(neural_network(images[11]).data))\n",
        "second_network.eval()\n",
        "print(np.argmax(second_network(images[11]).data))\n",
        "print(labels[11])"
      ],
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor(3)\n",
            "tensor(4)\n",
            "tensor(6)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/container.py:139: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  input = module(input)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "62r4vIlTByMr"
      },
      "source": [
        "#Conclusion: Neither of the Models has the best accuracy on the Fashion Minst Data most likely due to the \n",
        "# very simple architecture but the loss goes down for both and they have the same number of trainable paremters\n",
        "# Future Steps: Implement more complicated architecture similar to Convolution Layers!"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}